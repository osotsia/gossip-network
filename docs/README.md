
# System Architecture

## 1. Architectural Philosophy

This system is designed around three core principles: a Service-Oriented Architecture, Domain-Centric Design, and the Actor Model for concurrency.

*   **Service-Oriented Architecture (SOA):** The application is decomposed into independent, long-running services, each with a single, well-defined responsibility. The primary services are the `Engine` (core logic), `Transport` (networking), and `ApiServer` (web interface). These services do not share memory and communicate exclusively through asynchronous message passing via channels. This design promotes loose coupling, making services easier to test, maintain, and reason about in isolation.

*   **Domain-Centric Design:** The core concepts, data structures, and business rules of the application are isolated in a `domain` module. This module is self-contained and has no dependencies on infrastructure concerns like networking or web frameworks. The cryptographic identity of a node is treated as a fundamental part of the domain, not as an external utility. This separation ensures the core logic is pure, portable, and highly testable.

*   **Actor Model:** Each service is implemented as an actor running in its own lightweight Tokio task. An actor owns its state, processes messages from an inbox (an `mpsc` channel), and sends messages to other actors. This model provides a robust framework for managing concurrency and state, eliminating the need for complex locks or other manual synchronization primitives at the service level.

## 2. Folder and File Structure

The project is organized into modules that reflect the service-oriented architecture.

```
gossip-network/
├── Cargo.toml
├── orchestrator.sh     # Script for deploying a local cluster with advanced topology.
├── certs/
│   └── (Auto-generated by orchestrator)
├── frontend/
│   └── (Svelte 5 project for the web visualizer)
└── src/
    ├── main.rs         # Binary entry point. Instantiates and runs `App`.
    ├── lib.rs          # Public library facade. Exports `App`, `config`, `error`.
    │
    ├── app.rs          # Defines the main `App` struct. Manages service lifecycle and orchestration.
    ├── config.rs       # Configuration loading and data structures.
    ├── domain.rs       # Core data types, cryptographic identity, and operations.
    ├── error.rs        # Custom, typed error enum for the library using `thiserror`.
    │
    ├── engine/         # Core application logic and state management.
    │   ├── mod.rs      # Defines and runs the `Engine` service/actor. Owns state.
    │   └── protocol.rs # Implements the gossip propagation algorithm.
    │
    ├── transport/      # P2P network transport layer (QUIC).
    │   ├── mod.rs      # Defines and runs the `Transport` service/actor. Manages the QUIC endpoint.
    │   ├── connection.rs # Connection caching, establishment, and stream handling logic.
    │   └── tls.rs      # TLS configuration using a private PKI.
    │
    └── api/            # External API for the web visualizer.
        ├── mod.rs      # Defines and runs the `ApiServer` service. Sets up Axum routes.
        ├── protocol.rs # Defines the WebSocket message protocol (snapshot/update).
        └── ws.rs       # Implements the WebSocket connection logic, including state snapshot and delta updates.
```

## 3. Core Components (Services)

The application's runtime is composed of three primary services orchestrated by the `App` struct.

### `engine` Service
The `Engine` is the brain of a node. It encapsulates the application's core logic and state.

*   **Responsibilities:**
    *   Maintaining the node's view of the network state (a map of all known nodes and their latest telemetry).
    *   Tracking the state of active P2P connections based on events from the `Transport` service.
    *   Periodically generating this node's own signed telemetry data.
    *   Processing validated inbound messages from the `Transport` service.
    *   Applying the gossip protocol to decide which peers to forward new information to.
    *   Publishing state changes (including active connections) for consumption by the `ApiServer`.
*   **Inputs:** Receives `InboundMessage` and `ConnectionEvent` objects from the `Transport` service via `mpsc` channels.
*   **Outputs:** Sends `TransportCommand` objects to the `Transport` service. Broadcasts `NetworkState` updates via a `watch` channel.

### `transport` Service
The `Transport` service is the node's interface to the network. It handles all low-level peer-to-peer communication over QUIC.

*   **Responsibilities:**
    *   Binding a QUIC endpoint to a network socket.
    *   Establishing and accepting secure peer connections using unique TLS certificates signed by a private Certificate Authority.
    *   Managing a connection cache to reuse existing connections.
    *   Reporting connection lifecycle events (`PeerConnected`, `PeerDisconnected`) back to the `Engine`.
    *   Limiting concurrent inbound streams via a semaphore to prevent resource exhaustion attacks.
*   **Inputs:** Receives `TransportCommand` objects (e.g., `SendMessage`) from the `Engine`.
*   **Outputs:** Sends validated `InboundMessage` objects and `ConnectionEvent` objects to the `Engine`.

### `api` Service
The `ApiServer` provides the HTTP and WebSocket interface for the web-based visualizer. It is a read-only component.

*   **Responsibilities:**
    *   Serving the static Svelte 5 frontend application files (HTML, CSS, JS).
    *   Accepting WebSocket connections from clients.
    *   Sending a full snapshot of the current network state to newly connected clients, followed by incremental delta updates for all subsequent changes.
*   **Inputs:** Subscribes to `NetworkState` updates from the `Engine` via a `watch` channel.
*   **Outputs:** Sends serialized JSON data over WebSocket connections.

## 4. Key Supporting Modules

*   **`domain.rs`**: The lingua franca of the system. It contains the core data structures (`NodeId`, `TelemetryData`, `SignedMessage`) and consolidates cryptographic identity management (`Identity`, signing, verification).
*   **`app.rs`**: The application orchestrator. The `App` struct is responsible for initializing all services, wiring their communication channels together, and managing graceful shutdown.
*   **`error.rs`**: Defines a comprehensive, typed `Error` enum for the entire library using `thiserror`, providing clear, structured error handling.

## 5. Data Flow

### 5.1. Lifecycle of a Gossiped Message

1.  **Generation (Node A):** A periodic timer in Node A's `Engine` fires. The `Engine` creates a `GossipPayload`, signs it to produce a `SignedMessage`, and updates its own local state.
2.  **Command (Node A):** The `Engine` wraps the `SignedMessage` and a target peer's address (Node B) in a `TransportCommand::SendMessage` and sends it to its `Transport` service.
3.  **Transmission (Node A):** The `Transport` service receives the command. It retrieves a cached QUIC connection to Node B, serializes the `SignedMessage`, and writes the bytes to a network stream.
4.  **Reception (Node B):** Node B's `Transport` service receives the bytes from the QUIC stream and deserializes them into a `SignedMessage`.
5.  **Forwarding (Node B):** The `Transport` service wraps the message in an `InboundMessage` and sends it to its `Engine`.
6.  **Processing (Node B):** Node B's `Engine` receives the `InboundMessage`. It verifies the signature and checks if the message contains newer information than what it already knows about Node A.
7.  **Propagation (Node B):** If the information is new, the `Engine` updates its state, publishes the new `NetworkState`, and then invokes the gossip protocol to select a random subset of its *other* peers (e.g., Node C) to forward the original `SignedMessage` to, repeating the cycle from Step 2.

### 5.2. Visualizer State Synchronization

The API uses an efficient snapshot-plus-delta protocol to keep the frontend updated.

1.  **Client Connection:** A browser connects to the `/ws` endpoint on the designated visualizer node.
2.  **Initial Snapshot:** The `ApiServer` waits for the `Engine` to initialize its state. It then serializes the *entire* current `NetworkState` into a `WebSocketMessage::Snapshot` and sends it to the client. The server stores this state as `last_sent_state`.
3.  **State Change & Delta Calculation:** When the `Engine`'s state changes (e.g., a node is added, a connection is lost), it publishes a new `NetworkState`. The `ApiServer` receives this update and compares it to `last_sent_state`, generating a list of specific change events (`NodeAdded`, `ConnectionStatus`, etc.).
4.  **Delta Transmission:** Each change event is wrapped in a `WebSocketMessage::Update` and sent to the client. The frontend applies these small, incremental updates rather than re-processing the entire state.
5.  **State Update:** The server updates its `last_sent_state`, ready for the next change.

## 6. Security Model

Trust is established at two independent layers: transport and application.

*   **Transport Layer (TLS/QUIC with Private PKI):** Network-level trust is managed by a private Public Key Infrastructure. The `orchestrator.sh` script generates a root Certificate Authority (CA) and issues a **unique** TLS certificate and private key to each node, signed by the CA. QUIC connections are only permitted between nodes presenting a valid certificate from this PKI. This prevents unauthorized machines from joining the network or performing man-in-the-middle attacks.

*   **Application Layer (ED25519 Signatures):** Data-level trust is managed by cryptographic signatures. Each node has a persistent `Identity` based on an ED25519 keypair, where the public key serves as its globally unique `NodeId`. Every piece of gossiped telemetry is signed by the originator's private key. Receiving nodes verify the signature against the originator's `NodeId`. This guarantees message authenticity and integrity, preventing a compromised but network-authorized node from forging messages on behalf of others.

## 7. Multi-Node Deployment and Orchestration

A network is formed by running multiple instances of the application, managed by the `orchestrator.sh` script. This script automates the complex setup of a local cluster.

*   **PKI and Certificate Management:** The script first generates a root CA using `minica` if one does not exist. It then generates a unique, signed TLS certificate/key pair for each node, automating the setup of the transport-layer security.
*   **Configuration and Topology Generation:** The script creates a working directory for each node and generates a unique `config.toml`. It assigns unique ports and a `community_id`. Based on user-provided intra- and inter-community connection ratios, it calculates and assigns a `bootstrap_peers` list to each node, enabling the creation of structured network topologies.
*   **Designated Visualizer Node:** The script employs a "Designated Visualizer" pattern. It enables the `[visualizer]` configuration and copies the `frontend/dist` files *only* for `node-0`. This makes the UI accessible at a predictable address (`127.0.0.1:8080`) while displaying the network state from that specific node's perspective.
*   **Process Management:** It launches all nodes as background processes and manages their PIDs for graceful shutdown via a `trap` on script exit.

## 8. Testing Strategy

The architecture supports a multi-layered testing strategy:

*   **Unit Tests:** Placed directly within modules (`#[cfg(test)]`), these test pure, stateless logic, such as cryptographic operations in `domain.rs` and the peer selection algorithm in `engine/protocol.rs`.
*   **Integration Tests:** Located in `tests/integration/`, these tests validate the interaction between multiple services. The `network.rs` test is an end-to-end test that spins up multiple full application instances, configures them to connect, and verifies correct state propagation over a real (local) network via a WebSocket client. This provides the highest level of confidence in the system's correctness.